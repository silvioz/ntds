{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [NTDS'17] demo 7: data exploration and visualisation\n",
    "[ntds'17]: https://github.com/mdeff/ntds_2017\n",
    "\n",
    "Michael Defferrard and Hermina Petric Maretic, based on [exercises from the 2016 edition of NTDS](https://github.com/mdeff/ntds_2016)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Organising and looking at data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join('..', 'data', 'credit_card_defaults.csv')\n",
    "data = pd.read_csv(filename, index_col = 0)\n",
    "attributes = data.columns.tolist()\n",
    "\n",
    "# Tansform from numerical to categorical variable.\n",
    "data['SEX'] = data['SEX'].astype('category')\n",
    "data['SEX'].cat.categories = ['MALE', 'FEMALE']\n",
    "data['MARRIAGE'] = data['MARRIAGE'].astype('category')\n",
    "data['MARRIAGE'].cat.categories = ['UNK', 'MARRIED', 'SINGLE', 'OTHERS']\n",
    "data['EDUCATION'] = data['EDUCATION'].astype('category')\n",
    "data['EDUCATION'].cat.categories = ['UNK', 'GRAD SCHOOL', 'UNIVERSITY', 'HIGH SCHOOL', 'OTHERS', 'UNK1', 'UNK2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[:6, ['LIMIT', 'SEX', 'EDUCATION', 'MARRIAGE', 'AGE', 'DEFAULT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[:5, 4:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[:5, 11:23]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export as an [HTML table](./subset.html) for manual inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[:1000].to_html('subset.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Data Cleaning\n",
    "\n",
    "Problems come in two flavours:\n",
    "\n",
    "1. Missing data, i.e. unknown values.\n",
    "1. Errors in data, i.e. wrong values.\n",
    "\n",
    "The actions to be taken in each case is highly **data and problem specific**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: marital status\n",
    "1. According to dataset description, it should either be 1 (married), 2 (single) or 3 (others).\n",
    "1. But we find some 0 (previously transformed to `UNK`).\n",
    "1. Let's *assume* that 0 represents errors when collecting the data and that we should remove those clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['MARRIAGE'].value_counts())\n",
    "data = data[data['MARRIAGE'] != 'UNK']\n",
    "data['MARRIAGE'] = data['MARRIAGE'].cat.remove_unused_categories()\n",
    "print('\\nWe are left with {} clients\\n'.format(data.shape))\n",
    "print(data['MARRIAGE'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: education\n",
    "1. It should either be 1 (graduate school), 2 (university), 3 (high school) or 4 (others).\n",
    "1. But we find some 0, 5 and 6 (previously transformed to `UNK`, `UNK1` and `UNK2`).\n",
    "1. Let's *assume* these values are dubious, but do not invalidate the data and keep them as they may have some predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['EDUCATION'].value_counts())\n",
    "data.loc[data['EDUCATION'] == 'UNK1', 'EDUCATION'] = 'UNK'\n",
    "data.loc[data['EDUCATION'] == 'UNK2', 'EDUCATION'] = 'UNK'\n",
    "data['EDUCATION'] = data['EDUCATION'].cat.remove_unused_categories()\n",
    "print(data['EDUCATION'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Data statistics\n",
    "\n",
    "* Get descriptive statistics.\n",
    "* Plot informative figures.\n",
    "* Verify some intuitive correlations.\n",
    "\n",
    "### 3.1 Descriptive statistics\n",
    "Let's get first some descriptive statistics of our numerical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes_numerical = ['LIMIT', 'AGE']\n",
    "attributes_numerical.extend(attributes[11:23])\n",
    "data.loc[:, attributes_numerical].describe().astype(np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot an histogram of the ages, so that we get a better impression of who our clients are. That may even be an end goal, e.g. if your marketing team asks which customer groups to target.\n",
    "\n",
    "Then a boxplot of the bills, which may serve as a verification of the quality of the acquired data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[:, 'AGE'].plot.hist(bins=20, figsize=(15,5))\n",
    "ax = data.iloc[:, 11:17].plot.box(logy=True, figsize=(15,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Check a hypotesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple **question**: which proportion of our clients default ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage = data['DEFAULT'].value_counts()[1] / data.shape[0] * 100\n",
    "print('Percentage of defaults: {:.2f}%'.format(percentage))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another **question**: who's more susceptible to default, males or females ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observed = pd.crosstab(data['SEX'], data['DEFAULT'], margins=True)\n",
    "observed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like females are better risk. Let's verify with a Chi-Squared test of independance, using [scipy.stats](http://docs.scipy.org/doc/scipy/reference/stats.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "_, p, _, expected = stats.chi2_contingency(observed.iloc[:2,:2])\n",
    "print('p-value = {:.2e}'.format(p))\n",
    "print('expected values:\\n{}'.format(expected))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Intuition**: people who pay late present a higher risk of defaulting. Let's verify!\n",
    "Verifying some intuitions will also help you to identify mistakes. E.g. it would be suspicious if that intuition is not verified in the data: did we select the right column, or did we miss-compute a result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = data.groupby('DELAY1').mean()\n",
    "corr = data['DEFAULT'].corr(data['DELAY1'], method='pearson')\n",
    "group['DEFAULT'].plot(grid=True, title='Pearson correlation: {:.4f}'.format(corr), figsize=(15,5));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = data.groupby('AGE').mean()\n",
    "group['LIMIT'].plot(grid=True, title='Mean credit card limit per age', figsize=(15,5));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['AGE'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Statistical modelling\n",
    "\n",
    "[Statsmodels](http://statsmodels.sourceforge.net/) is similar to scikit-learn, with much stronger emphasis on parameter estimation and (statistical) testing. It is similar in spirit to other statistical packages such as [R](https://www.r-project.org), [SPSS](http://www.ibm.com/analytics/us/en/technology/spss), [SAS](http://www.sas.com/de_ch/home.html) and [Stata](http://www.stata.com). That split reflects the [two statistical modeling cultures](http://projecteuclid.org/euclid.ss/1009213726): (1) Statistics, which want to know how well a given model fits the data, and what variables \"explain\" or affect the outcome, and (2) Machine Learning, where the main supported task is chosing the \"best\" model for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Back to numeric values.\n",
    "data['SEX'].cat.categories = [-1, 1]\n",
    "data['SEX'] = data['SEX'].astype(np.int)\n",
    "data['MARRIAGE'].cat.categories = [-1, 1, 0]\n",
    "data['MARRIAGE'] = data['MARRIAGE'].astype(np.int)\n",
    "data['EDUCATION'].cat.categories = [-2, 2, 1, 0, -1]\n",
    "data['EDUCATION'] = data['EDUCATION'].astype(np.int)\n",
    "\n",
    "data['DEFAULT'] = data['DEFAULT'] * 2 - 1  # [0,1] --> [-1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observations and targets.\n",
    "X = data.values[:,:23]\n",
    "y = data.values[:,23]\n",
    "n, d = X.shape\n",
    "print('The data is a {} with {} samples of dimensionality {}.'.format(type(X), n, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Fit the Ordinary Least Square regression model.\n",
    "results = sm.OLS(y, X).fit()\n",
    "\n",
    "# Inspect the results.\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Data visualisation\n",
    "Data visualization is a key aspect of exploratory data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start slowly, let's make a static line plot from some time series. Reproduce the plots below using:\n",
    "1. The procedural API of [matplotlib](http://matplotlib.org), the main data visualization library for Python. Its procedural API is similar to matlab and convenient for interactive work.\n",
    "2. [Pandas](http://pandas.pydata.org), which wraps matplotlib around his DataFrame format and makes many standard plots easy to code. It offers many [helpers for data visualization](http://pandas.pydata.org/pandas-docs/version/0.19.1/visualization.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Random time series.\n",
    "n = 1000\n",
    "rs = np.random.RandomState(42)\n",
    "data = rs.randn(n, 4).cumsum(axis=0)\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(data[:, 0], label='A')\n",
    "plt.plot(data[:, 1], '.-k', label='B')\n",
    "plt.plot(data[:, 2], '--m', label='C')\n",
    "plt.plot(data[:, 3], ':', label='D')\n",
    "plt.legend(loc='upper left')\n",
    "plt.xticks(range(0, 1000, 50))\n",
    "plt.ylabel('Value')\n",
    "plt.xlabel('Day')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pd.date_range('1/1/2000', periods=n)\n",
    "df = pd.DataFrame(data, index=idx, columns=list('ABCD'))\n",
    "df.plot(figsize=(15,5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Frequency\n",
    "\n",
    "A frequency plot is a graph that shows the pattern in a set of data by plotting how often particular values of a measure occur. They often take the form of an [histogram](https://en.wikipedia.org/wiki/Histogram) or a [box plot](https://en.wikipedia.org/wiki/Box_plot).\n",
    "\n",
    "[Seaborn](http://seaborn.pydata.org/) is a statistical visualization library based on matplotlib. It provides a high-level interface for drawing attractive statistical graphics. Its advantage is that you can modify the produced plots with matplotlib, so you loose nothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "df = sns.load_dataset('iris', data_home=os.path.join('..', 'data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "g = sns.distplot(df['petal_width'], kde=True, rug=False, ax=axes[0])\n",
    "g.set(title='Distribution of petal width')\n",
    "\n",
    "g = sns.boxplot('species', 'petal_width', data=df, ax=axes[1])\n",
    "g.set(title='Distribution of petal width by species');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Correlation\n",
    "\n",
    "[Scatter plots](https://en.wikipedia.org/wiki/Scatter_plot) are very much used to assess the correlation between 2 variables. Pair plots are then a useful way of displaying the pairwise relations between variables in a dataset.\n",
    "\n",
    "Use the seaborn `pairplot()` function to analyze how separable is the iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df, hue=\"species\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Dimensionality reduction\n",
    "\n",
    "Humans can only comprehend up to 3 dimensions (in space, then there is e.g. color or size), so [dimensionality reduction](https://en.wikipedia.org/wiki/Dimensionality_reduction) is often needed to explore high dimensional datasets. Analyze how separable is the iris dataset by visualizing it in a 2D scatter plot after reduction from 4 to 2 dimensions with two popular methods:\n",
    "1. The classical [principal componant analysis (PCA)](https://en.wikipedia.org/wiki/Principal_component_analysis).\n",
    "2. [t-distributed stochastic neighbor embedding (t-SNE)](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding).\n",
    "\n",
    "**Hints**:\n",
    "* t-SNE is a stochastic method, so you may want to run it multiple times.\n",
    "* The easiest way to create the scatter plot is to add columns to the pandas DataFrame, then use the Seaborn `swarmplot()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "X = pca.fit_transform(df.values[:, :4])\n",
    "df['pca1'] = X[:, 0]\n",
    "df['pca2'] = X[:, 1]\n",
    "\n",
    "tsne = TSNE(n_components=2)\n",
    "X = tsne.fit_transform(df.values[:, :4])\n",
    "df['tsne1'] = X[:, 0]\n",
    "df['tsne2'] = X[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "sns.swarmplot(x='pca1', y='pca2', data=df, hue='species', ax=axes[0])\n",
    "sns.swarmplot(x='tsne1', y='tsne2', data=df, hue='species', ax=axes[1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Graph visualisation\n",
    "You've already worked with Networkx to make basic graph plots. Other useful resources are [Gephi](https://gephi.org/), which allows for a nicer and more artistic approach, and [Plotly](https://plot.ly/python/network-graphs/), useful for data visualisation as well as graph drawing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import time\n",
    "import networkx as nx\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 20\n",
    "T = 50\n",
    "er = nx.erdos_renyi_graph(20,0.3)\n",
    "er.remove_nodes_from(nx.isolates(er))  # removing isolated nodes\n",
    "n = len(er)\n",
    "nx.draw(er)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time series data can also live on the nodes of our graph (e.g. temperatures in cities). Create random time series data for our graph and plot them on a graph with node size representing the signal value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also experiment with different layouts for drawing the graph. The default layout positions nodes using the Fruchterman-Reingold [force-directed algorithm](https://en.wikipedia.org/wiki/Force-directed_graph_drawing) (spring layout). Draw the graph using the spectral layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_eps = 1;\n",
    "epsilon = stats.norm.rvs(scale = sigma_eps, size=[n,T])  # noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_graph_time_series(x):\n",
    "    pos = nx.spectral_layout(er)  # we have to fix the position of nodes\n",
    "\n",
    "    l = 0\n",
    "    for i in range(0,T):\n",
    "        ax.clear()\n",
    "        nx.draw_networkx(er, ax=ax, pos=pos, node_size=x[:,i]*100)\n",
    "        ax.text(0,0,i)\n",
    "        fig.canvas.draw()\n",
    "        time.sleep(0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plt.ion()\n",
    "fig.show()\n",
    "fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_graph_time_series(epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last experiment we plotted noise. Create two events on your graph at times 10 and 30. Each event should be centered in a randomly selected node and increase the signal value of that node by 50. Both events should then propagate through the graph with the heat kernel model. When an event occurs, color the source node of the event to a different color to help visualise it better. Draw the graph using the spring layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_x(Lapl):\n",
    "    x = epsilon.copy()\n",
    "    peak = np.zeros((n,2))\n",
    "    for i in range(1,T):\n",
    "        l = 0\n",
    "        while (t[l] < i):\n",
    "            peak[source[l]][l] = 1\n",
    "            event = np.dot(scipy.linalg.expm((t[l]-i)*Lapl), peak[:,l])\n",
    "            x[:,i] = x[:,i] + 50*event\n",
    "            l += 1\n",
    "            if l >= 2:\n",
    "                break\n",
    "    return x\n",
    "\n",
    "t = [10, 30]\n",
    "source = stats.randint.rvs(low=0, high=n, size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lapl = nx.normalized_laplacian_matrix(er)\n",
    "Lapl = Lapl.todense()\n",
    "x = create_x(Lapl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_graph_diffusion(x):\n",
    "    pos = nx.spring_layout(er)  # we have to fix the position of nodes\n",
    "    node_col = ['r'] * T\n",
    "    l = 0\n",
    "    for i in range(0,T):\n",
    "        ax.clear()\n",
    "        if l<2:\n",
    "            if i == t[l]:\n",
    "                node_col[source[l]] = 'b'\n",
    "                l = l+1\n",
    "        nx.draw_networkx(er, ax=ax, node_color = node_col, pos=pos, node_size=x[:,i]*100)\n",
    "        ax.text(0,0,i)\n",
    "        fig.canvas.draw()\n",
    "        time.sleep(0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.ion()\n",
    "fig.show()\n",
    "fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_graph_diffusion(x)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 1
}
